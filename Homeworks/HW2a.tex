% Created 2015-02-11 Wed 21:56
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage{methodshw}
\usepackage{booktabs}
\providecommand{\alert}[1]{\textbf{#1}}

\title{8004 Homework 2}
\author{Nooreen Dabbish}
\date{\today}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs Org-mode version 7.9.3f}}

\begin{document}

\maketitle


\section{Suppose that we are working under the Gauss-Markov model}
\label{sec-1}

\[ \mathbf{Y} = \mathbf{X\beta} + \mathbf{\epsilon} \]
where $E(\epsilon) = \mathbf{0}$ and
$var(\epsilon)=\sigma^2\mathbf{I}$. Let $\hat{\mathbf{Y}}$ be the
ordinary least square estimator of $\mathbf{Y}$.
 
\subsection{Show that $\hat{\mathbf{Y}}$ and $\mathbf{Y} - \hat{\mathbf{Y}}$ are uncorrelated.}
\label{sec-1-1}


Let $\hat{\mathbf{Y}} = P_X Y = X(X'X)^{-}X'Y$.

First, note that $\hat{\mathbf{Y}}$ and $\mathbf{Y} -
\hat{\mathbf{Y}}$ are orthogonal.

\[
\hat{\mathbf{Y}}'(\mathbf{Y} - \hat{\mathbf{Y}}) = (\mathbf{P_X Y})'(\mathbf{Y} - \mathbf{P_x Y})\\
                                                 = \mathbf{Y}'
                                                 \mathbf{P_X}' (\mathbf{Y} -
                                                 \mathbf{P_x Y})\\
                                                 =
                                                 \mathbf{Y}'\mathbf{P_X
                                                 Y} -
                                                 \mathbf{Y}'\mathbf{P_X}'\mathbf{P_X
                                                 Y}\\
                                                 =
                                                 \mathbf{Y}'\mathbf{P_X}\mathbf{Y} -
                                                 \mathbf{Y}'\mathbf{P_X}\mathbf{Y}\\
                                                 =\mathbf{0}
\]

Therefore, the expectation 
\[
E(\hat{\mathbf{Y}}'(\mathbf{Y} -
\hat{\mathbf{Y}}))=E(\mathbf{0})=\mathbf{0}
\]

\[
E(\hat{\mathbf{Y}})=E(\mathbf{X\hat{\beta}})\\
                   =X E(\hat{\beta})\\
                   =X\beta
\]

\[
E(\mathbf{Y} - \hat{\mathbf{Y}})= \mathbf{X\beta} - \mathbf{X\beta} \\
                                = \mathbf{0}
\]

This shows $\hat{\mathbf{Y}}$ and $\mathbf{Y} - \hat{\mathbf{Y}}$ are
uncorrelated because $E(\hat{\mathbf{Y}}(\mathbf{Y} -
\hat{\mathbf{Y}}) - E(\hat{\mathbf{Y}})E(\mathbf{Y} -
\hat{\mathbf{Y}})$ is zero.
\subsection{Show that}
\label{sec-1-2}

$$ E\{(\mathbf{Y} - \hat{\mathbf{Y}})^T(\mathbf{Y} -
\hat{\mathbf{Y}})\} = \sigma^2\{n-\mathrm{rank}(\mathbf{X})\}.$$
You may use Theorem 5.2a of R\&S.


Theorem 5.2a states:
If \textbf{y} is a random vector with mean \textbf{$\mu$} and covariance
matrix \textbf{$\Sigma$} and if \textbf{A} is a symmetric matrix of contants, then 
\[
E(\mathbf{y'Ay}) = tr(\mathbf{A\Sigma}) +\mathbf{\mu' A \mu}.
\]


We can apply Theorem 5.2a with \textbf{A} = \textbf{I} and $\mathbf{y} =
\mathbf{Y} - \hat{\mathbf{Y}}$. From part a) above, our mean \textbf{$\mu$}
is \textbf{0}. 
\[
Var(\mathbf{Y} - \hat{\mathbf{Y}}) = Var ((\mathbf{I} -
\mathbf{P_X})Y) \\
= \sigma^2(\mathbf{I} - \mathbf{P_X})
\]

The trace of an \emph{nxn} identity matrix \textbf{I} is n, and the trace a
projection matrix is the rank of target space, $tr(P_X) = rank(X)$.

This gives the desired result:
$$ E\{(\mathbf{Y} - \hat{\mathbf{Y}})^T(\mathbf{Y} -
\hat{\mathbf{Y}})\} = \sigma^2\{n-\mathrm{rank}(\mathbf{X})\}.$$
\section{Consider the one-way ANOVA model $y_{ij} = \mu + \tau_i + \epsilon_{ij}$ for the jth individual of the ith group.}
\label{sec-2}

Suppose there are 4 treatments (groups) and the sample sizes are 
respectively 2,1,1,2 for treatments.
Now suppose that $\mathbf{Y} = (y_{11}, y_{12}, y_{21}, y_{31},
y_{41}, y_{42})^{T} = (2, 1, 4, 6, 3, 5)^{T}$ contains the observations.

Use R and weighted generalized least squares to find an appropriate 
estimate for
$$E(\mathbf{Y})\,\mathrm{and}\,
\begin{pmatrix}
1 & 1 & 0 & 0 & 0 \\
1 & 0 & 1 & 0 & 0 \\
1 & 0 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 & 1 
\end{pmatrix}\mathbf{\beta}$$
in the Aiken model with $\mathrm{var}(\epsilon) = \mathbf{V}$ for two
cases where
 
\subsection{$\mathbf{V} = \mathbf{V}_1 = \mathrm{diag}(1,9,9,1,1,9)$}
\label{sec-2-1}



The full model described in this question in 
$\mathbf{Y}=\mathbf{X\beta}+\epsilon$ matrix form is:

\[
\begin{pmatrix}
y_{11} \\ y_{12}\\ y_{21}\\ y_{31}\\ y_{41}\\ y_{42}
\end{pmatrix} = 
\begin{pmatrix} 
2\\ 1\\ 4\\ 6\\ 3\\ 5
\end{pmatrix} = 
\begin{pmatrix}
1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
1 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 
\end{pmatrix}  
\begin{pmatrix}
\mu \\ \tau_1 \\ \tau_2 \\ \tau_3 \\ \tau_4 \\ \delta_{11}
\\ \delta_{12} \\ \delta_{21} \\ \delta_{31} \\ \delta_{41} \\ \delta_{42}
\end{pmatrix} + 
\begin{pmatrix}
\epsilon_{11} \\ \epsilon_{12}\\ \epsilon_{21}\\ \epsilon_{31}\\ \epsilon_{41}\\ \epsilon_{42}
\end{pmatrix}
\]

We have $var(\epsilon) = \sigma^2 \mathbf{V}$, so we must re-write
the model in terms of $\mathbf{U} = \mathbf{V}^{-1/2}\mathbf{Y}$ as
follows:

\begin{align*}
\mathbf{V} &= \mathbf{V}^{1/2} \mathbf{V}^{1/2},\, \text{V is a
diagonal matrix}\\
\mathrm{Let }\, \mathbf{U}& =\mathbf{V}^{-1/2}Y\\ E(\mathbf{U}) &= \mathbf{V}^{-1/2}E{Y} = \mathbf{V}^{-1/2}\mathbf{X\beta}\\
&= \mathbf{W\beta}\\
Var(\mathbf{U}) &= \mathbf{V}^{-1/2} Var(\mathbf{Y})\mathbf{V}^{-1/2}\\
                &= \sigma^2 \mathbf{V}^{-1/2} \mathbf{V}\mathbf{V}^{-1/2}\\
                &= \sigma^2 \mathbf{I}\\
\epsilon^{\star} &= \mathbf{V}^{-1/2} \mathbf{\epsilon}
\end{align*}

This gives us $\mathbf{U} = \mathbf{W\beta} + \epsilon^{\star}$, where
the Gauss-Markov assumptions hold for \textbf{U}.

We first calculate \textbf{V$^{\mathrm{-1/2}}$}:

\begin{align*}
\mathbf{V} &= \mathbf{V}_1 = \mathrm{diag}(1,9,9,1,1,9)\\
\text{So,}\, \mathbf{V}^{1/2}_1 &= \mathrm{diag}(1,3,3,1,1,3)\\
             \mathbf{V}^{-1/2}_1 &= \mathrm{diag}(1,1/3,1/3,1,1,1/3)\\
\end{align*}

We can check this in R,

\begin{verbatim}
V <- diag(c(1,9,9,1,1,9))
Vhi <- solve(V^(1/2))
lvector(Vhi)
\end{verbatim}

\[
\mathbf{V}^{-1/2} = 
% latex table generated in R 3.0.2 by xtable 1.7-4 package
% Tue Feb 10 14:46:43 2015
\begin{pmatrix}{}
  1.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  0.00 & 0.33 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  0.00 & 0.00 & 0.33 & 0.00 & 0.00 & 0.00 \\ 
  0.00 & 0.00 & 0.00 & 1.00 & 0.00 & 0.00 \\ 
  0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 \\ 
  0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.33 \\ 
  \end{pmatrix}
\]

\begin{align*}
\mathbf{U} &= \mathbf{V}^{-1/2} \mathbf{Y} = 
\begin{pmatrix}
y_{11} \\ \frac{1}{3} y_{12}\\ \frac{1}{3} y_{21}\\ y_{31}\\ y_{41}\\ \frac{1}{3}y_{42}
\end{pmatrix} = 
\begin{pmatrix} 
2\\ \frac{1}{3}\\ \frac{4}{3}\\ 6\\ 3\\ \frac{5}{3}
\end{pmatrix}
\end{align*}

Checking \textbf{U} in R gives:

\begin{verbatim}
Y <- matrix(c(2, 1, 4, 6, 3, 5), nrow=6, ncol=1)
U <- Vhi %*% Y
lvector(U)
\end{verbatim}

\[
\mathbf{U} =
% latex table generated in R 3.0.2 by xtable 1.7-4 package
% Wed Feb 11 19:13:05 2015
\begin{pmatrix}{}
  2.00 \\ 
  0.33 \\ 
  1.33 \\ 
  6.00 \\ 
  3.00 \\ 
  1.67 \\ 
  \end{pmatrix}
\]

\begin{align*}
\mathbf{W} &= \mathbf{V}^{-1/2}\mathbf{X} \\
           &= \mathrm{diag}(1,1/3,1/3,1,1,1/3)\begin{pmatrix}
1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
1 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
1 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 \\
1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 1 
\end{pmatrix}  \\
 &= 
\begin{pmatrix}{}
  1 & 1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\ 
  \frac{1}{3} & \frac{1}{3} & 0 & 0 & 0 & 0 & \frac{1}{3} & 0 & 0 & 0 & 0 \\ 
  \frac{1}{3} & 0 & \frac{1}{3} & 0 & 0 & 0 & 0 & \frac{1}{3} & 0 & 0 & 0 \\ 
  1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\ 
  1 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 0 \\ 
  \frac{1}{3} & 0 & 0 & 0 & \frac{1}{3} & 0 & 0 & 0 & 0 & 0 & \frac{1}{3} \\ 
  \end{pmatrix}
\end{align*}

Checking \textbf{W} in R gives:

\begin{verbatim}
X <- matrix(c(rep(1,6),
              1,1,0,0,0,0,
              0,0,1,0,0,0,
              0,0,0,1,0,0,
              0,0,0,0,1,1,
              rep(c(1,rep(0,6)),5),1
              ),nrow = 6,byrow=FALSE)
W <- Vhi %*% X
lvector(W)
\end{verbatim}

\[
\mathbf{W} =
% latex table generated in R 3.0.2 by xtable 1.7-4 package
% Tue Feb 10 15:19:08 2015
\begin{pmatrix}{}
  1.00 & 1.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  0.33 & 0.33 & 0.00 & 0.00 & 0.00 & 0.00 & 0.33 & 0.00 & 0.00 & 0.00 & 0.00 \\ 
  0.33 & 0.00 & 0.33 & 0.00 & 0.00 & 0.00 & 0.00 & 0.33 & 0.00 & 0.00 & 0.00 \\ 
  1.00 & 0.00 & 0.00 & 1.00 & 0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 & 0.00 \\ 
  1.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 & 0.00 & 0.00 & 0.00 & 1.00 & 0.00 \\ 
  0.33 & 0.00 & 0.00 & 0.00 & 0.33 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.33 \\ 
  \end{pmatrix}
\]
\subsubsection{Solving $\mathbf{U} = \mathbf{W\beta} + \mathbf{\epsilon}$ for $\hat{\mathbf{U}}$}
\label{sec-2-1-1}


\begin{align*}
\hat{\mathbf{U}} &=
\mathbf{W}(\mathbf{W}'\mathbf{W})^{-}\mathbf{W}'\mathbf{Y}
\end{align*}

\begin{verbatim}
Uhat <- W %*% ginv(t(W) %*% W) %*% t(W) %*% U
Uhat
W %*% ginv(t(W) %*% W) %*% t(W)
\end{verbatim}
\subsubsection{}
\subsection{V$_2$}
\label{sec-2-2}

\[ 
\mathbf{V} = \mathbf{V}_2 = 
\begin{pmatrix}
1 & 1 & 0 & 0 & 0 & 0 \\
1 & 9 & 0 & 0 & 0 & 0 \\
0 & 0 & 9 & -1& 0 & 0 \\
0 & 0 & -1& 1 & 0 & 0 \\
0 & 0 & 0 & 0 & 1 & -1 \\
0 & 0 & 0 & 0 & -1 & 9
\end{pmatrix}
\]

\end{document}
